{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from mysql.connector import errors\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Database credentials\n",
    "DB_CONNECTION = os.getenv('DB_CONNECTION')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_DATABASE = os.getenv('DB_DATABASE')\n",
    "DB_USERNAME = os.getenv('DB_USERNAME')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn = mysql.connector.connect(\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB_DATABASE,\n",
    "        user=DB_USERNAME,\n",
    "        password=DB_PASSWORD\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT * FROM foqus;\")\n",
    "    rows = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    print(rows)\n",
    "except errors.ProgrammingError as err:\n",
    "    print(f\"Error connecting to MySQL: {err}\")\n",
    "finally:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visua/20210601_NRK_Dagsrevyen.mp4\n",
      "Downloaded visua/2020717_Eurosport_ODD_VIK_1.mp4 to /Users/thomasrye/Documents/github/Foocus-Logodetection2.mp4\n",
      "Downloaded visua/2020717_Eurosport_ODD_VIK_1.mp4 to /Users/thomasrye/Documents/github/Foocus-Logodetection2.mp4\n"
     ]
    }
   ],
   "source": [
    "import connect as co\n",
    "import os\n",
    "\n",
    "co.connect_to_s3()\n",
    "\n",
    "files = co.list_files_in_folder(\"visua\")\n",
    "\n",
    "print(files[2])\n",
    "\n",
    "# get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "file_name = \"2.mp4\"\n",
    "\n",
    "co.download_file_from_s3(files[1], cwd + \"/videos/2.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/thomasrye/.cache/torch/hub/pytorch_vision_main\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from detect_utils import predict, draw_boxes\n",
    "import torch\n",
    "\n",
    "model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "cap = cv2.VideoCapture('videos/1.mp4')\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('output.mp4', fourcc, fps, (width, height))\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video.\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Call the predict function to get the bounding boxes, class names, and labels.\n",
    "    boxes, classes, labels = predict(frame, model, torch.device('cpu'), detection_threshold=0.5)\n",
    "    \n",
    "    # Draw the bounding boxes on the image.\n",
    "    image = draw_boxes(boxes, classes, labels, frame)\n",
    "    \n",
    "    # Write the frame with bounding boxes to the output video.\n",
    "    out.write(image)\n",
    "    \n",
    "    # Display the output image with bounding boxes.\n",
    "    cv2.imshow('frame', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from detect_utils import predict, draw_boxes\n",
    "from coco_names import COCO_INSTANCE_CATEGORY_NAMES as coco_names\n",
    "from model import get_model\n",
    "\n",
    "# Define the torchvision image transforms.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the pre-trained ResNet model.\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.fpn.inner_blocks.0.1.weight\", \"backbone.fpn.inner_blocks.0.1.bias\", \"backbone.fpn.inner_blocks.0.1.running_mean\", \"backbone.fpn.inner_blocks.0.1.running_var\", \"backbone.fpn.inner_blocks.0.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.1.1.weight\", \"backbone.fpn.inner_blocks.1.1.bias\", \"backbone.fpn.inner_blocks.1.1.running_mean\", \"backbone.fpn.inner_blocks.1.1.running_var\", \"backbone.fpn.inner_blocks.1.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.2.1.weight\", \"backbone.fpn.inner_blocks.2.1.bias\", \"backbone.fpn.inner_blocks.2.1.running_mean\", \"backbone.fpn.inner_blocks.2.1.running_var\", \"backbone.fpn.inner_blocks.2.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.3.1.weight\", \"backbone.fpn.inner_blocks.3.1.bias\", \"backbone.fpn.inner_blocks.3.1.running_mean\", \"backbone.fpn.inner_blocks.3.1.running_var\", \"backbone.fpn.inner_blocks.3.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.0.1.weight\", \"backbone.fpn.layer_blocks.0.1.bias\", \"backbone.fpn.layer_blocks.0.1.running_mean\", \"backbone.fpn.layer_blocks.0.1.running_var\", \"backbone.fpn.layer_blocks.0.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.1.1.weight\", \"backbone.fpn.layer_blocks.1.1.bias\", \"backbone.fpn.layer_blocks.1.1.running_mean\", \"backbone.fpn.layer_blocks.1.1.running_var\", \"backbone.fpn.layer_blocks.1.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.2.1.weight\", \"backbone.fpn.layer_blocks.2.1.bias\", \"backbone.fpn.layer_blocks.2.1.running_mean\", \"backbone.fpn.layer_blocks.2.1.running_var\", \"backbone.fpn.layer_blocks.2.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.3.1.weight\", \"backbone.fpn.layer_blocks.3.1.bias\", \"backbone.fpn.layer_blocks.3.1.running_mean\", \"backbone.fpn.layer_blocks.3.1.running_var\", \"backbone.fpn.layer_blocks.3.1.num_batches_tracked\", \"rpn.head.conv.1.0.weight\", \"rpn.head.conv.1.0.bias\", \"roi_heads.box_head.0.0.weight\", \"roi_heads.box_head.0.1.weight\", \"roi_heads.box_head.0.1.bias\", \"roi_heads.box_head.0.1.running_mean\", \"roi_heads.box_head.0.1.running_var\", \"roi_heads.box_head.0.1.num_batches_tracked\", \"roi_heads.box_head.1.0.weight\", \"roi_heads.box_head.1.1.weight\", \"roi_heads.box_head.1.1.bias\", \"roi_heads.box_head.1.1.running_mean\", \"roi_heads.box_head.1.1.running_var\", \"roi_heads.box_head.1.1.num_batches_tracked\", \"roi_heads.box_head.2.0.weight\", \"roi_heads.box_head.2.1.weight\", \"roi_heads.box_head.2.1.bias\", \"roi_heads.box_head.2.1.running_mean\", \"roi_heads.box_head.2.1.running_var\", \"roi_heads.box_head.2.1.num_batches_tracked\", \"roi_heads.box_head.3.0.weight\", \"roi_heads.box_head.3.1.weight\", \"roi_heads.box_head.3.1.bias\", \"roi_heads.box_head.3.1.running_mean\", \"roi_heads.box_head.3.1.running_var\", \"roi_heads.box_head.3.1.num_batches_tracked\", \"roi_heads.box_head.5.weight\", \"roi_heads.box_head.5.bias\". \n\tsize mismatch for roi_heads.box_predictor.cls_score.weight: copying a param with shape torch.Size([91, 1024]) from checkpoint, the shape in current model is torch.Size([33, 1024]).\n\tsize mismatch for roi_heads.box_predictor.cls_score.bias: copying a param with shape torch.Size([91]) from checkpoint, the shape in current model is torch.Size([33]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.weight: copying a param with shape torch.Size([364, 1024]) from checkpoint, the shape in current model is torch.Size([132, 1024]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.bias: copying a param with shape torch.Size([364]) from checkpoint, the shape in current model is torch.Size([132]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load the trained weights.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m./models/bigdata.pth\u001b[39;49m\u001b[39m'\u001b[39;49m, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)))\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[39m# Define the input and output video paths.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FasterRCNN:\n\tMissing key(s) in state_dict: \"backbone.fpn.inner_blocks.0.0.bias\", \"backbone.fpn.inner_blocks.1.0.bias\", \"backbone.fpn.inner_blocks.2.0.bias\", \"backbone.fpn.inner_blocks.3.0.bias\", \"backbone.fpn.layer_blocks.0.0.bias\", \"backbone.fpn.layer_blocks.1.0.bias\", \"backbone.fpn.layer_blocks.2.0.bias\", \"backbone.fpn.layer_blocks.3.0.bias\", \"roi_heads.box_head.fc6.weight\", \"roi_heads.box_head.fc6.bias\", \"roi_heads.box_head.fc7.weight\", \"roi_heads.box_head.fc7.bias\". \n\tUnexpected key(s) in state_dict: \"backbone.fpn.inner_blocks.0.1.weight\", \"backbone.fpn.inner_blocks.0.1.bias\", \"backbone.fpn.inner_blocks.0.1.running_mean\", \"backbone.fpn.inner_blocks.0.1.running_var\", \"backbone.fpn.inner_blocks.0.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.1.1.weight\", \"backbone.fpn.inner_blocks.1.1.bias\", \"backbone.fpn.inner_blocks.1.1.running_mean\", \"backbone.fpn.inner_blocks.1.1.running_var\", \"backbone.fpn.inner_blocks.1.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.2.1.weight\", \"backbone.fpn.inner_blocks.2.1.bias\", \"backbone.fpn.inner_blocks.2.1.running_mean\", \"backbone.fpn.inner_blocks.2.1.running_var\", \"backbone.fpn.inner_blocks.2.1.num_batches_tracked\", \"backbone.fpn.inner_blocks.3.1.weight\", \"backbone.fpn.inner_blocks.3.1.bias\", \"backbone.fpn.inner_blocks.3.1.running_mean\", \"backbone.fpn.inner_blocks.3.1.running_var\", \"backbone.fpn.inner_blocks.3.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.0.1.weight\", \"backbone.fpn.layer_blocks.0.1.bias\", \"backbone.fpn.layer_blocks.0.1.running_mean\", \"backbone.fpn.layer_blocks.0.1.running_var\", \"backbone.fpn.layer_blocks.0.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.1.1.weight\", \"backbone.fpn.layer_blocks.1.1.bias\", \"backbone.fpn.layer_blocks.1.1.running_mean\", \"backbone.fpn.layer_blocks.1.1.running_var\", \"backbone.fpn.layer_blocks.1.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.2.1.weight\", \"backbone.fpn.layer_blocks.2.1.bias\", \"backbone.fpn.layer_blocks.2.1.running_mean\", \"backbone.fpn.layer_blocks.2.1.running_var\", \"backbone.fpn.layer_blocks.2.1.num_batches_tracked\", \"backbone.fpn.layer_blocks.3.1.weight\", \"backbone.fpn.layer_blocks.3.1.bias\", \"backbone.fpn.layer_blocks.3.1.running_mean\", \"backbone.fpn.layer_blocks.3.1.running_var\", \"backbone.fpn.layer_blocks.3.1.num_batches_tracked\", \"rpn.head.conv.1.0.weight\", \"rpn.head.conv.1.0.bias\", \"roi_heads.box_head.0.0.weight\", \"roi_heads.box_head.0.1.weight\", \"roi_heads.box_head.0.1.bias\", \"roi_heads.box_head.0.1.running_mean\", \"roi_heads.box_head.0.1.running_var\", \"roi_heads.box_head.0.1.num_batches_tracked\", \"roi_heads.box_head.1.0.weight\", \"roi_heads.box_head.1.1.weight\", \"roi_heads.box_head.1.1.bias\", \"roi_heads.box_head.1.1.running_mean\", \"roi_heads.box_head.1.1.running_var\", \"roi_heads.box_head.1.1.num_batches_tracked\", \"roi_heads.box_head.2.0.weight\", \"roi_heads.box_head.2.1.weight\", \"roi_heads.box_head.2.1.bias\", \"roi_heads.box_head.2.1.running_mean\", \"roi_heads.box_head.2.1.running_var\", \"roi_heads.box_head.2.1.num_batches_tracked\", \"roi_heads.box_head.3.0.weight\", \"roi_heads.box_head.3.1.weight\", \"roi_heads.box_head.3.1.bias\", \"roi_heads.box_head.3.1.running_mean\", \"roi_heads.box_head.3.1.running_var\", \"roi_heads.box_head.3.1.num_batches_tracked\", \"roi_heads.box_head.5.weight\", \"roi_heads.box_head.5.bias\". \n\tsize mismatch for roi_heads.box_predictor.cls_score.weight: copying a param with shape torch.Size([91, 1024]) from checkpoint, the shape in current model is torch.Size([33, 1024]).\n\tsize mismatch for roi_heads.box_predictor.cls_score.bias: copying a param with shape torch.Size([91]) from checkpoint, the shape in current model is torch.Size([33]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.weight: copying a param with shape torch.Size([364, 1024]) from checkpoint, the shape in current model is torch.Size([132, 1024]).\n\tsize mismatch for roi_heads.box_predictor.bbox_pred.bias: copying a param with shape torch.Size([364]) from checkpoint, the shape in current model is torch.Size([132])."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the trained weights.\n",
    "model.load_state_dict(torch.load('./models/bigdata.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define the input and output video paths.\n",
    "input_path = 'videos/1.mp4'\n",
    "output_path = 'output.mp4'\n",
    "\n",
    "# Open the input video file.\n",
    "cap = cv2.VideoCapture(input_path)\n",
    "\n",
    "# Get the input video's frame rate and dimensions.\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Create a VideoWriter object to write the output video.\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Loop over each frame in the input video.\n",
    "while True:\n",
    "    # Read a frame from the input video.\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Call the predict function to get the bounding boxes, class names, and labels.\n",
    "    boxes, classes, labels = predict(frame, model, torch.device('cpu'), detection_threshold=0.5)\n",
    "    \n",
    "    # Draw the bounding boxes on the image.\n",
    "    image = draw_boxes(boxes, classes, labels, frame)\n",
    "    \n",
    "    # Write the frame with bounding boxes to the output video.\n",
    "    out.write(image)\n",
    "    \n",
    "    # Display the output image with bounding boxes.\n",
    "    cv2.imshow('frame', image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture and VideoWriter objects, and close the OpenCV window.\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from utils import (\n",
    "    collate_fn,\n",
    "    get_transform,\n",
    "    myOwnDataset,\n",
    ")\n",
    "import config\n",
    "from eval import evaluate_model\n",
    "import model\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Load the PyTorch model\n",
    "model =  model.get_model(device=device, model_name='v2')\n",
    "model.load_state_dict(torch.load('models/bigData.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "# # Create test Dataset\n",
    "# testing_dataset = myOwnDataset(\n",
    "#     root=config.data_dir, annotation=config.test_coco, transforms=get_transform()\n",
    "# )\n",
    "\n",
    "# # Testing DataLoader\n",
    "# testing_loader = torch.utils.data.DataLoader(\n",
    "#     testing_dataset,\n",
    "#     batch_size=config.test_batch_size,\n",
    "#     shuffle=config.train_shuffle_dl,\n",
    "#     num_workers=config.num_workers_dl,\n",
    "#     collate_fn=collate_fn,\n",
    "# )\n",
    "\n",
    "\n",
    "# # Evaluate the model\n",
    "# output = evaluate_model(model.to(device), device, testing_loader, \"notrans\")\n",
    "\n",
    "# # Print the output\n",
    "# print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
